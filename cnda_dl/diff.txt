#!/usr/bin/env python						#!/usr/bin/env python

'''								'''
Script to download MRI sessions from the CNDA			Script to download MRI sessions from the CNDA
Authors:							Authors:
    Joey Scanga (scanga@wustl.edu)				    Joey Scanga (scanga@wustl.edu)
    Ramone Agard (rhagard@wustl.edu)				    Ramone Agard (rhagard@wustl.edu)
'''								'''

from glob import glob						from glob import glob
from matplotlib.ticker import EngFormatter			from matplotlib.ticker import EngFormatter
from pathlib import Path					from pathlib import Path
from pyxnat import Interface					from pyxnat import Interface
import argparse							import argparse
import logging							import logging
import os							import os
import progressbar						import progressbar
import re							import re
import shlex							import shlex
import shutil							import shutil
import subprocess						import subprocess
import sys							import sys
import xml.etree.ElementTree as et				import xml.etree.ElementTree as et
import zipfile							import zipfile
from textwrap import dedent				      <

def handle_dir_creation(dir_title, path_str):		      |	log_path = f"{Path.home().as_posix()}/cnda-dl.log"
    '''							      |	logging.basicConfig(level=logging.INFO,
    Handles creation of specified directories that don't exis |	                    handlers=[
							      >	                        logging.FileHandler(log_path)
							      >	                    ])
							      >	sout_handler = logging.StreamHandler(stream=sys.stdout)
							      >	logger = logging.getLogger(__name__)
							      >	logger.addHandler(sout_handler)
							      >	logger.info("Starting cnda-dl...")
							      >	logger.info(f"Log will be stored at {log_path}")

    :param dir_title: how the directory is denoted in prompt  <
    :type dir_title: str				      <
    :param path_str: string representing path to new director <
    :type path_str: str					      <
    '''								'''
							      >	dir_title: how the directory is denoted in prompt messages
							      >	path_str: string representing path to new directory
							      >	'''
							      >	def handle_dir_creation(dir_title, path_str):
    prompt, prompt_chosen = input(f"{dir_title} directory doe	    prompt, prompt_chosen = input(f"{dir_title} directory doe
    while not prompt_chosen:					    while not prompt_chosen:
        if len(prompt) < 1 or prompt[0].lower() not in 'yn':	        if len(prompt) < 1 or prompt[0].lower() not in 'yn':
            logger.info("Invalid response.")			            logger.info("Invalid response.")
            prompt = input(f"{dir_title} directory does not e	            prompt = input(f"{dir_title} directory does not e
        elif prompt[0].lower() == 'y':				        elif prompt[0].lower() == 'y':
            new_path_str = Path(path_str)			            new_path_str = Path(path_str)
            new_path_str.mkdir(parents=True)			            new_path_str.mkdir(parents=True)
            prompt_chosen = True				            prompt_chosen = True
            logger.info(f"{dir_title} directory created at {n	            logger.info(f"{dir_title} directory created at {n
            del new_path_str					            del new_path_str
        elif prompt[0].lower() == 'n':				        elif prompt[0].lower() == 'n':
            logger.info(f"Chose to not create new {dir_title}	            logger.info(f"Chose to not create new {dir_title}
            sys.exit(0)						            sys.exit(0)
        else:							        else:
            logger.info("Invalid response.")			            logger.info("Invalid response.")
            prompt = input(f"{dir_title} directory does not e	            prompt = input(f"{dir_title} directory does not e

def main():						      |	# Function to format number of bytes per file into MB, GB, et
							      >	fmt = EngFormatter('B')
							      >
    parser = argparse.ArgumentParser(				parser = argparse.ArgumentParser(
        prog = "cnda-dl",					    prog = "cnda-dl",
        description= "download cnda data directly to wallace"	    description= "download cnda data directly to wallace",
    )								)
    parser.add_argument('session_list',				parser.add_argument('session_list', 
                        nargs= "+",				                    nargs= "+", 
                        help="List of either subject labels o	                    help="List of either subject labels or ex
    parser.add_argument("-d", "--dicom_dir",			parser.add_argument("-d", "--dicom_dir", 
                        help="Path to the directory the dicom	                    help="Path to the directory the dicom fil
                        required=True)				                    required=True)
    parser.add_argument("-x", "--xml_dir",			parser.add_argument("-x", "--xml_dir", 
                        help="Path to the directory the sessi	                    help="Path to the directory the session x
    parser.add_argument("-e", "--experiment_id",		parser.add_argument("-e", "--experiment_id",
                        help="Query by CNDA experiment identi	                    help="Query by CNDA experiment identifier
                        action='store_true')			                    action='store_true')
    parser.add_argument("-p", "--project_id",			parser.add_argument("-p", "--project_id", 
                        help="Specify the project ID to narro	                    help="Specify the project ID to narrow do
    parser.add_argument("-s", "--scan_number",			parser.add_argument("-s", "--scan_number", 
                        help="Select the scan number to start	                    help="Select the scan number to start the
    parser.add_argument("-n", "--ignore_nordic_volumes",	parser.add_argument("-n", "--ignore_nordic_volumes", 
                        help="Don't download a NORDIC_VOLUMES	                    help="Don't download a NORDIC_VOLUMES fol
                        action='store_true')			                    action='store_true')
    parser.add_argument("--map_dats", 				parser.add_argument("--map_dats", 
                        help=					                    help=
                        dedent("""The path to a directory con |	                    """The path to a directory containting .d
                        means that all data is already availa	                    means that all data is already available 
                        run dcmdat2niix"""))		      |	                    run dcmdat2niix""")
    parser.add_argument("-ks","--keep_short_runs",		parser.add_argument("-ks","--keep_short_runs",
                        action="store_true",			                    action="store_true",
                        help="Flag to indicate that runs stop	                    help="Flag to indicate that runs stopped 
    parser.add_argument("--skip_unusable",			parser.add_argument("--skip_unusable", 
                        help="Don't download any scans marked	                    help="Don't download any scans marked as 
                        action='store_true')			                    action='store_true')
    args = parser.parse_args()					args = parser.parse_args()

    logger = logging.getLogger(__name__)		      <
    log_path = f"{Path.home().as_posix()}/cnda-dl.log"	      <
    logging.basicConfig(level=logging.INFO,		      <
                        handlers=[			      <
                            logging.StreamHandler(),	      <
                            logging.FileHandler(log_path)     <
                        ])				      <
    logger.info("Starting cnda-dl...")			      <
    logger.info(f"Log will be stored at {log_path}")	      <
    # Function to format number of bytes per file into MB, GB <
    fmt = EngFormatter('B')				      <
							      <
    if not args.experiment_id and not hasattr(args, 'project_	if not args.experiment_id and not hasattr(args, 'project_id')
        raise RuntimeError("ERROR: Must specify --project_id 	    raise RuntimeError("ERROR: Must specify --project_id (or 

    session_list = args.session_list				session_list = args.session_list
    dicom_path = Path(args.dicom_dir).as_posix()		dicom_path = Path(args.dicom_dir).as_posix()
    if hasattr(args, 'xml_dir') and args.xml_dir != None:	if hasattr(args, 'xml_dir') and args.xml_dir != None:
        xml_path = Path(args.xml_dir).as_posix()		    xml_path = Path(args.xml_dir).as_posix()
    else:							else:
        xml_path = dicom_path					    xml_path = dicom_path
    scan_num = None						scan_num = None

    if hasattr(args, "scan_number"):				if hasattr(args, "scan_number"):
        assert len(session_list) == 1, "ERROR: Scan number is	    assert len(session_list) == 1, "ERROR: Scan number is spe
        scan_num = args.scan_number				    scan_num = args.scan_number

    dat_dir = None						dat_dir = None
    if hasattr(args, "map_dats") and args.map_dats != None an	if hasattr(args, "map_dats") and args.map_dats != None and os
        dat_dir = Path(args.map_dats).as_posix()		    dat_dir = Path(args.map_dats).as_posix()
								    
							      >
    if not os.path.isdir(dicom_path):				if not os.path.isdir(dicom_path):
        handle_dir_creation("DICOM", dicom_path)		    handle_dir_creation("DICOM", dicom_path)
    if not os.path.isdir(xml_path):				if not os.path.isdir(xml_path):
        handle_dir_creation("XML", xml_path)			    handle_dir_creation("XML", xml_path)
    
    central = None						central = None
    if not dat_dir:						if not dat_dir:
        central = Interface(server="https://cnda.wustl.edu/")	    central = Interface(server="https://cnda.wustl.edu/")

    for session in session_list:				for session in session_list:
							      >	    if not dat_dir:
        logger.info(f"Starting download of session {session}.	        logger.info(f"Starting download of session {session}.
        query_params = {}					        query_params = {}
        if hasattr(args, 'project_id'):				        if hasattr(args, 'project_id'):
            query_params['project_id'] = args.project_id	            query_params['project_id'] = args.project_id
        if args.experiment_id:					        if args.experiment_id:
            query_params['experiment_id'] = session		            query_params['experiment_id'] = session
        else:							        else:
            query_params['subject_label'] = session		            query_params['subject_label'] = session

        exp = central.array.mrsessions(**query_params)		        exp = central.array.mrsessions(**query_params)
        							        
        if len(exp) == 0:					        if len(exp) == 0:
            raise RuntimeError("ERROR: query returned JsonTab |	            logger.error("CNDA query returned JsonTable objec
							      >	            raise RuntimeError("ERROR: CNDA query returned Js

        if len(exp) > 1:					        if len(exp) > 1:
            raise RuntimeError("ERROR: query returned JsonTab |	            logger.error("CNDA query returned JsonTable objec
							      >	            raise RuntimeError("ERROR: CNDA query returned Js

        project_id = exp["project"]				        project_id = exp["project"]
        subject_id = exp["xnat:mrsessiondata/subject_id"]	        subject_id = exp["xnat:mrsessiondata/subject_id"]

        logger.info("Downloading session xml...")		        logger.info("Downloading session xml...")
        sub = central.select(f"/projects/{project_id}/subject	        sub = central.select(f"/projects/{project_id}/subject
        with open(xml_path+f"/{session}.xml", "w") as f:	        with open(xml_path+f"/{session}.xml", "w") as f:
            f.write(sub.get().decode())				            f.write(sub.get().decode())

        xml_path = xml_path + f"/{session}.xml"			        xml_path = xml_path + f"/{session}.xml"
        tree = et.parse(xml_path)				        tree = et.parse(xml_path)
        prefix = "{" + str(tree.getroot()).split("{")[-1].spl	        prefix = "{" + str(tree.getroot()).split("{")[-1].spl
        scan_xml_entries = tree.getroot().find(			        scan_xml_entries = tree.getroot().find(
            f"./{prefix}experiments/{prefix}experiment/{prefi	            f"./{prefix}experiments/{prefix}experiment/{prefi
        )							        )
        quality_pairs = {s.get("ID") : s.find(f"{prefix}quali	        quality_pairs = {s.get("ID") : s.find(f"{prefix}quali
                         for s in scan_xml_entries}		                        for s in scan_xml_entries}

        scans = central.select(f"/projects/{project_id}/exper	        scans = central.select(f"/projects/{project_id}/exper
							      >	        scans.sort(key=lambda x: int(x))
							      >	        logger.info(f"Found {len(scans)} scans for this sessi
							      >
							      >	        if scan_num:
							      >	                assert scan_num in scans, "Specified scan num
							      >	                sdex = scans.index(scan_num)
							      >	                scans = scans[sdex:]
							      >	                logger.info(f"Downloading scans for this sess

        if args.skip_unusable:					        if args.skip_unusable:
            i = 0						            i = 0
            while i < len(scans):				            while i < len(scans):
                if quality_pairs[scans[i]] == "unusable":	                if quality_pairs[scans[i]] == "unusable":
                    logger.info(f"Not downloading scan {scans	                    logger.info(f"Not downloading scan {scans
                    del scans[i]				                    del scans[i]
                else:						                else:
                    i += 1					                    i += 1

        if scan_num:					      <
            assert scan_num in scans, "Specified scan number  <
            sdex = scans.index(scan_num)		      <
            scans = scans[sdex:]			      <
							      <
            						      <
        # Get total number of files				        # Get total number of files
        total_file_count, cur_file_count = 0, 0			        total_file_count, cur_file_count = 0, 0
        for s in scans:						        for s in scans:
            files = central.select(f"/projects/{project_id}/e	            files = central.select(f"/projects/{project_id}/e
            total_file_count += len(files)			            total_file_count += len(files)
        
        logger.info(f"Total number of files: {total_file_coun	        logger.info(f"Total number of files: {total_file_coun
							      >	        # So log message does not interfere with format of th
							      >	        logger.removeHandler(sout_handler)
        with progressbar.ProgressBar(max_value=total_file_cou	        with progressbar.ProgressBar(max_value=total_file_cou
            for s in scans:					            for s in scans:
							      >	                logger.info(f"  Downloading scan {s}...")
                print(f"Downloading scan {s}...")		                print(f"Downloading scan {s}...")
                series_path = dicom_path+f"/{session}/{s}/DIC	                series_path = dicom_path+f"/{session}/{s}/DIC
                os.makedirs(series_path, exist_ok=True)		                os.makedirs(series_path, exist_ok=True)
                files = central.select(f"/projects/{project_i	                files = central.select(f"/projects/{project_i
                for f in files:					                for f in files:
                    print(f"\tFile {f.attributes()['Name']}, 	                    print(f"\tFile {f.attributes()['Name']}, 
                    f.get(series_path + "/" +f._uri.split("/"	                    f.get(series_path + "/" +f._uri.split("/"
                    cur_file_count += 1				                    cur_file_count += 1
                    bar.update(cur_file_count)			                    bar.update(cur_file_count)
							      >	        logger.addHandler(sout_handler)
							      >	        logger.info("Dicom download complete \n")

        if not args.ignore_nordic_volumes:			    if not args.ignore_nordic_volumes:
							      >	        nv = None
            # Check for NORDIC files in this session		        # Check for NORDIC files in this session
							      >	        if not dat_dir:
            nv = central.select(f"/projects/{project_id}/expe	            nv = central.select(f"/projects/{project_id}/expe
							      >	        else:
							      >	            nv = [dat_dir]
							      >
            if len(nv) == 0:					        if len(nv) == 0:
                logger.warning(f"No NORDIC_VOLUMES folder fou	            logger.warning(f"No NORDIC_VOLUMES folder found f
                continue					            continue
							      >	        logger.info("NORDIC files found for this session")

            # Check if NORDIC dat files can be converted usin	        # Check if NORDIC dat files can be converted using dc
            can_convert = True					        can_convert = True
            nii_path = f"{dicom_path}/{session}_nii"		        nii_path = f"{dicom_path}/{session}_nii"
            if shutil.which('dcmdat2niix') != None:		        if shutil.which('dcmdat2niix') != None:
                os.mkdir(nii_path)			      |	            os.makedirs(nii_path, exist_ok=True)
                logger.info(f"Combined .dcm & .dat files (.ni	            logger.info(f"Combined .dcm & .dat files (.nii.gz
            else:						        else:
                logger.info("dcmdat2niix not installed or has	            logger.info("dcmdat2niix not installed or has not
                can_convert = False				            can_convert = False
            unconverted_series = set()    			        unconverted_series = set()    
            
            # Create dict mapping series number to timestamp 	        # Create dict mapping series number to timestamp in t
							      >	        if dat_dir:
							      >	            xml_path = xml_path + f"/{session}.xml"
							      >	            tree = et.parse(xml_path)
							      >	            prefix = "{" + str(tree.getroot()).split("{")[-1]
							      >	            scan_xml_entries = tree.getroot().find(
							      >	                f"./{prefix}experiments/{prefix}experiment/{p
							      >	            )
            uid_to_id = {s.get("UID")[:-6] : s.get("ID") for 	        uid_to_id = {s.get("UID")[:-6] : s.get("ID") for s in
							      >
            for f in nv:					        for f in nv:
							      >	            dat_files = []
							      >	            if not dat_dir:
                zip_path = f"{dicom_path}/{session}/" + f._ur	                zip_path = f"{dicom_path}/{session}/" + f._ur
                logger.info(f"Downloading {zip_path.split('/'	                logger.info(f"Downloading {zip_path.split('/'
                f.get(zip_path)					                f.get(zip_path)
                unzip_path = zip_path[:-4]			                unzip_path = zip_path[:-4]
                with zipfile.ZipFile(zip_path, 'r') as zip_re	                with zipfile.ZipFile(zip_path, 'r') as zip_re
                    logger.info(f"Unzipping to {unzip_path}..	                    logger.info(f"Unzipping to {unzip_path}..
                    zip_ref.extractall(unzip_path)		                    zip_ref.extractall(unzip_path)
                dat_files = glob(unzip_path + "/*.dat")		                dat_files = glob(unzip_path + "/*.dat")
							      >	            else:
							      >	                dat_files = glob(f+"/*.dat")
                timestamp_to_dats = {t: [d for d in dat_files	            timestamp_to_dats = {t: [d for d in dat_files if 
                for timestamp, dats in timestamp_to_dats.item	            for timestamp, dats in timestamp_to_dats.items():
                    series_id = uid_to_id[timestamp]		                series_id = uid_to_id[timestamp]
                    series_path = f"{dicom_path}/{session}/{s	                series_path = f"{dicom_path}/{session}/{serie
                    for dat in dats:				                for dat in dats:
                        shutil.move(dat, series_path)		                    shutil.move(dat, series_path)

                    # Check if there's a mismatch between num	                # Check if there's a mismatch between number 
                    dcms = glob(series_path + "/*.dcm")		                dcms = glob(series_path + "/*.dcm")
                    if (len(dats) != 0) and (len(dats) != len	                if (len(dats) != 0) and (len(dats) != len(dcm
                        logger.warning(f"WARNING: number of .	                    logger.warning(f"WARNING: number of .dat 
                        logger.warning(f"\t.# of dats: {len(d |	                    logger.warning(f"\t# of dats: {len(dats)}
                        logger.warning(f"\t.# of dcms: {len(d |	                    logger.warning(f"\t# of dcms: {len(dcms)}
                        logger.warning(f"This mismatch may in |	                    if not args.keep_short_runs:
							      >	                        logger.warning(f"This mismatch may in
                        unconverted_series.add(series_id)	                        unconverted_series.add(series_id)
                        continue				                        continue

                    # Convert DICOM and DAT files to NIFTI	                # Convert DICOM and DAT files to NIFTI
                    if can_convert:				                if can_convert:
                        logger.info(f"Running dcmdat2niix on 	                    logger.info(f"Running dcmdat2niix on seri
                        dcmdat2niix_cmd = shlex.split(f"dcmda	                    dcmdat2niix_cmd = shlex.split(f"dcmdat2ni
                        with subprocess.Popen(dcmdat2niix_cmd	                    with subprocess.Popen(dcmdat2niix_cmd, st
                            while p.poll() == None:		                        while p.poll() == None:
                                text = p.stdout.read1().decod	                            text = p.stdout.read1().decode("u
                                print(text, end="", flush=Tru	                            print(text, end="", flush=True)
                            if p.poll() != 0:		      |	                        if p.poll() == 0:
                                logger.error(f"dcmdat2niix en |	                            logger.info(f"dcmdat2niix complet
                        print()				      |	                        else:
							      >	                            logger.error(f"dcmdat2niix ended 
							      >	                            unconverted_series.add(series_id)
							      >	                    # Remove incomplete files due to runs tha
							      >	                    if series_id in unconverted_series:
							      >	                        bad_files = glob(f"{nii_path}/*_pha.*
							      >	                        for f in bad_files:
							      >	                            os.remove(f)
                        
                if len(unconverted_series) > 0:			            if len(unconverted_series) > 0:
                    logger.warning(f"""				                logger.warning(f"""
                    The following series for session:{session |	                The following series for session:{session} we
                    not converted to NIFTI due to inconsisten |	                not converted to NIFTI due to either being sk
							      >	                Check these series for possibly corrupted Dat
                    {unconverted_series}			                {unconverted_series}
                                    """)		      |	                                \n""")
							      >
							      >	logger.info("\n...Downloads Complete")
            							        
              							          
